# Local Chatbot

A complete chatbot application with a FastAPI backend and Next.js frontend, designed to work with Ollama for local AI model inference.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚   Ollama        â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (Local AI)    â”‚
â”‚   Port: 3000    â”‚    â”‚   Port: 8000    â”‚    â”‚   Port: 11434   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

1. **Docker & Docker Compose** installed
2. **Ollama** running locally on port 11434
3. **Models** downloaded in Ollama:
   ```bash
   ollama pull gemma3:1b
   ollama pull qwen3:1.7b
   ```
4. **Environment Configuration**: Create a `.env` file in the `chatbot-api/` directory

### Setup Environment

Create a `.env` file in the `chatbot-api/` directory with the following content:

```bash
# chatbot-api/.env
OLLAMA_MODEL=qwen3:1.7b
OLLAMA_BASE_URL=http://host.docker.internal:11434
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
```

### Production Deployment

```bash
# Clone the repository
git clone <your-repo-url>
cd local-chatbot

# Create the environment file
cp chatbot-api/.env.example chatbot-api/.env
# Edit chatbot-api/.env with your settings

# Start the production stack
make prod

# Access the application
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Development Mode

```bash
# Start development stack with hot reloading
make dev

# View logs
make dev-logs

# Stop development stack
make dev-down
```

## ğŸ“‹ Available Commands

Run `make help` to see all available commands:

```bash
make help          # Show help
make prod          # Start production stack
make dev           # Start development stack
make build         # Build all images
make logs          # View logs
make clean         # Clean up everything
make health        # Check service health
```

## ğŸ”§ Configuration

### Environment Variables

The application uses environment variables defined in `chatbot-api/.env`:

**Backend Configuration (`chatbot-api/.env`):**

- `OLLAMA_BASE_URL`: Ollama server URL (default: `http://host.docker.internal:11434`)
- `OLLAMA_MODEL`: Default model (default: `qwen3:1.7b`)
- `API_HOST`: API host (default: `0.0.0.0`)
- `API_PORT`: API port (default: `8000`)
- `LOG_LEVEL`: Logging level (default: `INFO`, use `DEBUG` for development)

**Frontend Configuration:**

- `NEXT_PUBLIC_API_URL`: Backend API URL (configured in Docker Compose)

### Custom Configuration

Edit the `chatbot-api/.env` file to customize your setup:

```bash
# chatbot-api/.env
OLLAMA_BASE_URL=http://your-ollama-host:11434
OLLAMA_MODEL=your-preferred-model
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
```

## ğŸ¯ Features

### Backend (FastAPI)

- âœ… **Streaming Chat**: Real-time response streaming
- âœ… **Model Selection**: Dynamic model switching
- âœ… **Health Checks**: Service monitoring
- âœ… **CORS Support**: Frontend integration
- âœ… **OpenAPI Docs**: Auto-generated documentation

### Frontend (Next.js)

- âœ… **Modern UI**: Clean, responsive design
- âœ… **Real-time Streaming**: Live response updates
- âœ… **Model Selector**: Choose from available models
- âœ… **Thinking Models**: Special support for `<think>` tags
- âœ… **Dark Mode**: Automatic theme switching
- âœ… **Mobile Friendly**: Responsive design

### Thinking Models Support

Models that use `<think>` tags get special treatment:

- Collapsible thinking sections
- Visual indicators (âš¡ lightning bolt)
- Separate thinking from response content
- "Thinking..." animation during processing

## ğŸ³ Docker Services

### Production Stack (`docker-compose.yml`)

- **chatbot-api**: Production FastAPI backend
- **chatbot-web**: Production Next.js frontend
- **Health checks**: Automatic service monitoring
- **Networks**: Isolated container communication
- **Environment**: Uses `chatbot-api/.env` file

### Development Stack (`docker-compose.dev.yml`)

- **Hot reloading**: Live code updates
- **Volume mounts**: Direct file system access
- **Debug logging**: Enhanced development logs (LOG_LEVEL=DEBUG)
- **Environment**: Uses `chatbot-api/.env` file + debug overrides

## ğŸ“ Project Structure

```
local-chatbot/
â”œâ”€â”€ chatbot-api/              # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/             # API routes
â”‚   â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â””â”€â”€ models.py        # Data models
â”‚   â”œâ”€â”€ .env                 # Environment configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ chatbot-web/             # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/            # Next.js app router
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”œâ”€â”€ types/          # TypeScript types
â”‚   â”‚   â””â”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ docker-compose.yml       # Production stack
â”œâ”€â”€ docker-compose.dev.yml   # Development stack
â”œâ”€â”€ Makefile                # Convenience commands
â””â”€â”€ README.md               # This file
```

## ğŸ” API Endpoints

### Health Check

```bash
GET /api/v1/health
```

### List Models

```bash
GET /api/v1/models
```

### Chat (Streaming)

```bash
POST /api/v1/chat/stream
Content-Type: application/json

{
  "message": "Hello, how are you?",
  "model": "qwen3:1.7b",
  "conversation_history": []
}
```

## ğŸ› Troubleshooting

### Common Issues

1. **Missing .env file**

   ```bash
   # Create the environment file
   cp chatbot-api/.env.example chatbot-api/.env
   # Edit with your settings
   ```

2. **Ollama Connection Failed**

   ```bash
   # Check if Ollama is running
   curl http://localhost:11434/api/tags

   # Start Ollama if needed
   ollama serve

   # Update OLLAMA_BASE_URL in chatbot-api/.env if needed
   ```

3. **Models Not Found**

   ```bash
   # List available models
   ollama list

   # Pull required models
   ollama pull gemma3:1b
   ollama pull qwen3:1.7b

   # Update OLLAMA_MODEL in chatbot-api/.env
   ```

4. **Port Conflicts**

   ```bash
   # Check what's using the ports
   lsof -i :3000
   lsof -i :8000

   # Update API_PORT in chatbot-api/.env or change ports in docker-compose.yml
   ```

5. **Docker Issues**

   ```bash
   # Clean up Docker resources
   make clean

   # Rebuild everything
   make build
   ```

### Health Checks

```bash
# Check service health
make health

# View logs
make logs

# Check individual services
curl http://localhost:8000/api/v1/health
curl http://localhost:3000
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with `make dev`
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local AI model inference
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
- [Next.js](https://nextjs.org/) for the frontend framework
- [Tailwind CSS](https://tailwindcss.com/) for styling
